{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\rupal\\anaconda3\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\rupal\\anaconda3\\lib\\site-packages (from python-docx) (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx as wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = wo.Document('Deepika Padukone-Data Analyst or Engineer.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'National University of Singapore M-Tech in Enterprise Business Analytics [Jan 2017 â€“ May 2018]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume.paragraphs[12].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rupal\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\rupal\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rupal\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\rupal\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rupal\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = []\n",
    "for paragraph in resume.paragraphs:\n",
    "     fullText.append(paragraph.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fulltext(text): \n",
    "    tokens=' '.join(text)    \n",
    "    return(tokens)\n",
    "\n",
    "resume1 = fulltext(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume1=pd.Series(resume1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume1=resume1.replace('[^A-Za-z0-9]+',' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstopwords=stopwords.words(\"English\") + ['play','will','within','use','make','way','across','need','care','help','high',\n",
    "                                           'must','area','state','people','member','may','well','using','etc','make',\n",
    "                                           'us','change','part','access','time','able','issue','task','duties','day','field',\n",
    "                                           'meet','hand','first','ensure','best','sexual','gender','world','race','regard',\n",
    "                                           'http','re','ad','self','highly','key','source','make','come','full','result','inc'\n",
    "                                           'build','end','color','work','role','family','date','team','equal','com',\n",
    "                                           'u','law','www','com','please','life','also','level','new','non','system','service',\n",
    "                                           'sex','age','email','job','pay','duty','take','ha','a','b','c','d','e','f','g','strong',\n",
    "                                          'base','status','offer','good','term','matter','open','get','person','ideal','place',\n",
    "                                          'title','go','inc','see','pay','hour','month','tx','ca','strong','-','+','\"',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens=[t.lower() for t in tokens]\n",
    "    tokens=[t for t in tokens if t not in string.punctuation]\n",
    "    tokens=[ t for t in tokens if (not t.isnumeric())]\n",
    "    tokens=[WNlemma.lemmatize(t) for t in tokens]\n",
    "    tokens=[word for word in tokens if word not in newstopwords]\n",
    "    tokens=' '.join(tokens)\n",
    "    return(tokens)\n",
    "\n",
    "resume2=resume1.apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume2=resume2.replace(['pl sql','masters','git github','github','microsoft word','ms word','powerpoint','power point','data governance','statistics','mathematics','computer science','data analyst','big data','modelling','microsoft office','ms office','java script','javascript','deep learning','tableau','engineering','datasets','data sets','dataset','data set','dashboards','dashboard','databases','data base',' sa ','regression','clustering','artificial intelligence',' ms ','mongodb','tensorflow','rapidminer','rainstor','elasticsearch','elastic search','airflow','kubernetes','tensor flow','air flow','power bi','scrum master','sisense','data studio','grafana','mapreduce','couchbase','blockchain','spotfire','chartio','metabase','cassandra','periscope data','dynamodb','firebase','big query',' teradata ','ad hoc','machine learning',' natural language processing ','decision tree','neural network','graduate','undergraduate','bachelor','analtyical','analytics','analysis','classification','business analyst','business intelligence'],['PLSQL','master','GIT','GIT','msword','msword','PPT','PPT','DAG','stats','math','cse','anali','bdata','model','mso','mso','jas','jas','deepl','tabl','engg','dset','dset','dset','dset','dashb','dashb','db','db',' sas ','resn','cstg','ai','master','mgd','tefw','rpdm','rst','elsch','elsch','afw','kbnts','tefw','afw','bi','scr','sss','dtd','gfn','mrc','ccb','bcn','spfr','chto','mtbse','ssdr','prsc','dymd','fibs','bqry',' tdata ','adhoc','maclrg',' nlp ','dctre','nurnet','grad','undergrad','bach','ana','anacs','anasis','clas','busst','busnc'],regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(text):\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=[t for t in tokens if len(t)<7]\n",
    "    tokens=set(tokens)\n",
    "    tokens=' '.join(tokens)\n",
    "    return(tokens)\n",
    "\n",
    "resume3=resume2.apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume3=resume3.replace(['DAG',' stats ',' math ',' cse ',' anali ',' bdata ',' model ',' mso ',' jas ',' deepl ',' tabl ',' engg ',' dset ',' dashb ',' db ',' teg ',' resn ',' cstg ',' mgd ',' tefw ',' rpdm ',' rst ',' elsch ',' afw ',' ai ',' kbnts ',' bi ',' scr ',' sss ',' dtd ',' gfn ',' mrc ',' ccb ',' bcn ',' spfr ',' chto ',' mtbse ',' ssdr ','data',' prsc ',' dymd ',' fibs ',' big ',' set ',' bqry ',' tdata ','adhoc','maclrg',' nlp ','dctre','nurnet',' grad ',' undergrad ',' bach ',' ana ',' anacs ',' anasis ',' clas ',' busst ',' busnc '],['datagovernance',' statistics ',' mathematics ',' computerscience ',' analyst ',' bigdata ',' modelling ',' msoffice ',' javascript ',' deeplearning ',' tableau ',' engineering ',' dataset ',' dashborads ',' databases ',' text mining ',' regression ',' clustering ',' mongodb ',' tensorflow ',' rapidminer ',' rainstor ',' elasticsearch ',' airflow ',' artificialintelligence ',' kubernetes ',' powerbi ',' scrum',' sisense ',' datastudio ',' grafana ',' mapreduce ',' couchbase ',' blockchain ',' spotfire ',' chartio ',' metabase ',' cassandra ','',' periscopedata',' dynamodb ',' firebase ','','',' bigquery ',' teradata ','adhoc','machine learning','natural launguage processing','decision tree','neural network',' graduate ',' undergraduate ',' bachelor ',' analytical ',' analytics ',' analysis ',' classification ',' businessanalyst',' businessintelligence '],regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'injury sia factor port gi google yield phone soundibm intern medium pulp qlik nu jan  clustering output analytics uld light excel spark poc tv PPT basis slide arc add reva gpp analysis gbm proof solver weekly final track ml flightigate mainly feed arima grossmanual anova series likely year machine learning adept horse june health learnt reefer vivid tableau hence r web versed making wa tech report spss design theory demand forest server degree modelling worked flask useful player query uat real user defect sheet type like gold price study built image agile paper region vessel love decision tree module august manner show text cost linear inter mining script search caused used test dashborads head till geo money rating tool sql psa engineering based view damage java cause detect neural network regression pca impact march goal bm bmsit hadoop safety domain edanatural launguage processingplan client rice sale bachelor april sensor known inform record delay python basic prize loan kpis random better word major case mobile farm skill hi msoffice fest'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'injury sia factor port gi google yield phone soundibm intern medium pulp qlik nu jan  clustering output analytics uld light excel spark poc tv PPT basis slide arc add reva gpp analysis gbm proof solver weekly final track ml flightigate mainly feed arima grossmanual anova series likely year machine learning adept horse june health learnt reefer vivid tableau hence r web versed making wa tech report spss design theory demand forest server degree modelling worked flask useful player query uat real user defect sheet type like gold price study built image agile paper region vessel love decision tree module august manner show text cost linear inter mining script search caused used test dashborads head till geo money rating tool sql psa engineering based view damage java cause detect neural network regression pca impact march goal bm bmsit hadoop safety domain edanatural launguage processingplan client rice sale bachelor april sensor known inform record delay python basic prize loan kpis random better word major case mobile farm skill hi msoffice fest'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume4=str(resume3[0])\n",
    "resume4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumefile=open('resume.txt',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumefile.write(resume4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
